12/31

So just kicking off this project, here we're implementing a version of 
AutoMap, after [1] Jana Diesner, Kathleen M Carley "AutoMap1.2 - Extract, analyze, represent, and compare mental models from texts" paper.
See: https://www.researchgate.net/publication/245564928_AutoMap12_-_Extract_analyze_represent_and_compare_mental_models_from_texts
for more detail. There is also a OAR software user guide involving Carley, [2] "ORA User's Guide 2018,"
see: http://www.casos.cs.cmu.edu/projects/ora/CMU-ISR-18-103.pdf that contains a lot of implementation related
information on AutoMap, including data types, data structures and methods, so it is also a useful reference.
Finally, see: [3] Carley, "Extracting Team Mental Models through Textual Analysis" (1997) for what I believe to believe
to be the original use of AutoMap.

Note: the paper specifically addresses card sorting, which is pretty interesting.
-
Since I want to get something out by mid Janurary that uses this I need to make this implementation very simple and quick.

Here's my NLP break down of [1]

Method and Models
* Map analysis is a type of "Network Text Analysis," encodes links among words and constructs networks thereof
with maps as "cognitively motivated representation of knowledge." <-- key difference from straight forward syntax tree
* AutoMap does network analysis (map analysis)
* From those adjacency matrices it analyzes the "meta matrix" and "sub-matrix"
* A concept is a single idea represented as a word, phrase
* A phrase is is two concepts and the (directed) relation between them
* A map is a network of statements

Input, How AutoMap codes texts as maps
* Takes in raw text
* 1) Makes a concept list, words with their frequencies (think, sklearn dictionary vectorizer)
* 2) User can modify text processing which changes the concept list
* 3) Pre-process text (stem words, etc; see sklearn stemming and alternatives)
* 4) Statement formation is specific to the type of network text analysis desired
* 5) ^ form the coding scheme (see: Section 4.2, Diesner, Lewis and Carley, 2003 on impact of)
...

Pro-processing
* 6) User can delete non relevant words, apply "generalization"
* 6.1 Generalization <-- Generating synoymns from word, taken as a "higher level" concept
    <-- a is-a relation (think, sematch is-a, knowledge base)
* 7) Can decide to use generalized concepts only or retain text concepts, or retain that text
that has no match to a higher level concept and also keep higher concepts and discard text matching those.

4.3 Statement formation
* 8) Operationalizes distance between concepts (think, sematch distance, word2vec distance)
* 9) Text Unit: run over a sentence, paragraph or whole text
* 10) Windows Size: defined as a count of concepts and concepts this count or less away are related
* 11) Directionality: Assuming reading left to right, directed relationship, otherwise unidirectional
* 12) Adjacency: Window Size sensitive to removed words or not; rhetorical adjacency consider distance
original text only <-- much of the description reflects standard NLP but this is unique
Note: Statement formation creates higher level concept pairs (this isn't explicitly mentioned)

4.4 Impact of statement formation on analysis results
See figure 4, nice chart. Basically, direct text grows like the logarithm, much like language itself,
and rhetorical styles grow logarithmically too except that Rhetorical Text apperas almost curvilinear
(which is interesting; is this a kind of compositional growth?)

Strong recomend to run with different coding schemes (like, how about all of them?)

Then, we can create a concept by concept adjacency matrix to represent the mental model and 
answer the following:
* What words do people use?
* How do people link the words they use?
* What words do people use in order to refer to more abstract themes?
* What themes do people envoke?
* How do people link themes they evoke?

Types of Analysis
Map Analysis
* Summary table of concept counts, etc

Meta-matrix
* Classifies nodes a level higher into: agent, knowledge, resources, tasks, and organizations associated to concepts

Sub-matrix
* "Distils sub-network from the meta-matrix"
* basically replaces concept with the word that connected to that concept
* seems pretty similar to Map analysis

We then do the union, different of both sets to answer how the mental models differ or not.
--
Okay so this is pretty simple, the author doesn't provide a psycholinguistic justification or basis
for method or experimental evidence correlating it indirect mental models but it's a very well cited paper.

my thoughts on a quick implementation:

Essentially what's happening here is distance measure on pairs of concepts (raw text or higher order level concepts)
This distance measure is derived from a representation of the text over a window function, where 
the text is just concepts or concepts + raw text.

I'll think through an efficient way to implement this tmmrw (?)
-----

okay, so what we want here is a function that takes in raw text and
outputs a set of (sparse) coordinated matrices that reprsent each possible
parameterization (generalization, retain (original) text, consider removed words and directionality)
this is over a fixed text unit and range of window sizes

Also use memoization to speed up look ups, speed up range of window size
And parallelize where possible (?) <-- probably premature; I know memoization will shortcut a lot of
memory look ups.

so this is a rought sketch of an implementation.
-
it also might be easer to compute somehting likethe cross product and then drop stuff we don't need?
well, since we're doing a lagged window function, that's probably linearly faster.
-
OKay I should be using spaCy (https://spacy.io/), it provides a lot of what I want to do despite being
a little bit on the heavy side.
-
OKauy so i need to rewrite the first unit test but the advantage here is that
the remaining unit test(s) will be much easier since spaCy does so much when it
parses documents

NOTE: we have to run `python3 -m spacy download en_core_web_sm` :(
--
okay so spaCy doesn't remove the stop words and when you define your own stop words it 
doesn't affect how lexemes are treated as stop words, wihch means you can make your own custom list
but it's harder or unclear to remove words like 'should' which are lexemes.

so this means I need to pass in the stop list and parse off of that. This is too bad because it means I can'that
I can't use .is_stop under custom lists
-
found WordnetAnnotator, didn't even know this was a thing. This will allow us to tag
words with their Wordnet cocnepts and we can remove sematch. Note: remove sematch
--
1/2

The above notes are somewhat anemic but the big take away is that the spacy-wordnet library
allows me to not need sematch and memoize because everything is a look up now. I still need NLTK 
for the wordnet portion.

I was able to get generalization workign quite nicely. Note that wordnet domains are not
as good as hypernyms, root_hypernyms for mental mapping under this algorithm.

I'm currently considering the "rhetorical adjacency" option, however since I
use wordnet as my theaurus it's so complete that no word will fail to match something
(pretty much). This means that the only words that fail to match will be those that 
I pick out by stopword (is_stop), or in the delete_list. 

Considering the logical states here, then, if someone choses
`want_higher_concepts=True, include_text_concepts=False`, they just want the higher concepts,
but then wants to construct statements with rhetorical adjacency
then we can do that over the included text concepts version and simply not form concept pairs/statements
with those words that are .is_stop or in the detele list. This reduction occurs
because NLTK hypernyms are so extensive and only delete list or stopwords remain.

okay, so given the above what I can actualy do is do a direct look up into nlp_en.vocab['to'].is_stop
which, I guess, is a simple filter that drops and then constrcuts the iterated pair wise list
or and also the revers of the pair list (when directionality is the same).
--

Okay so I'm rapidaly approach the point at which I need to consider a justification for parameter
selection. The best approach is just run everything.

The dataset has no paragraph level granuality so we can only look at the turn level or entire dialogue.
So a speed up for this is to do the sentence/turn level and thencombine them and find the
begninning and end of the remaining sentenences, ... actuallpy this await. yea
so whe we do, then, is to get the widnows size and begninngin of next sentence of each
turn to compute the text level concepts that were over looked by the sentence level.
We could also just memoize, which I'ms tarting tot hink is the better option here than
algorithmic approaches since the algorithmic approach (so far) doens't handle variation in
window size. Whoops i came up with one: take the combination of concepts under of the
max window size, store the offset with the conepts, then filter with offset subtraction to determine
what windows that comnbination falls under. THis is because the work at max window size include the work
at all asmaller sizes.

Some of this is hard to thing through directly because it touches upon what statistic i'm computing
and what i'm aggregating for the statistical test. I don't currently have that pinned down. I actually
saw this mental map overlap as a simple number betwen 0 and 1
-
and, actually, the rhetorical and direct methods (the only real curve ball here) can be addressed the same
way: calculate the hardest case, which is direct statement under maximum window size with text concepts.
Then filter by offset for window size and filter by .is_stop look up for rhetorical. Directionality
doesn't add any information here so I think that's it.

The text size is an issue in that what do we do with it; I can efficiently construct results but
am I comparing mental models at the paired sentence level? Then doing an average of averages, almost
like a monte carlo simulation over response pairs? Or am I instead doing the average over the entire dialoge?
--
so mental models in dialoge are incrementally constructed. so I thinking across spoken/written exchanges is appropriate
Of course, doing the whole thing mi ... hmm this relaly might not make sense. hard to say but
but! I can point to research that says that mental models are naturally incrementally constructed, I can
likely even point to psycholinguistic research too. This is a good justification, then, for averaging over
exchanges.
-

So all this means I'm still doing windows. We speak about 2 - 2.5 words a second and then average sentence
is 17 words (this is written though); well wahtever, we really just wanted to captur ethe lower and upper bound
so, in terms of window sizes, we could do from 2 to the span of the statement. This is fair.
Great.
And we're not going to do the whole text level, this means we only need to focus on sentences or statemetns. Great.

So then I think i've thought through the first level justifiations needed here; mental models constructed incrementally
so we measure the overlap between exchanges in dialgoue. There are two papers, 
"Mental models as a practical tool in the engineer’s toolbox" Sinreich et al (2005) and "How Influential Are Mental Models on Interaction Performance?
Exploring the Gap between Users’ and Designers’ Mental Models through a New Quantitative Method," Xie et al (2017)
that both use the same similarity method. I would cite both as a justification for using the method.
--
I could also consider a different similarity, a stock one but there are so many graph similarity methods and 
the one discussed above is realted to hierarchical process and cognitive modeling.
-
okay, so that was a big rabbit hole to go down. I should now implement

"Direct statement under maximum window size with text concepts, then filter
by offset for window size and filter by .is_stop look up for rhetorical"
--
so i'm going to returne a compressed representation such that teh cocnepts in a given
window size can be calcualted by adding all smaller window size concepts with it. this way
i'm not taking up more memory than I need. I do wonder if I'm over optimizing a bit but it 
does make it easier to read and debug stuff

I will reproduce this data for the rhetorical version of the response
-